IEEE-754 是第一版约定了各类不同型号、架构的计算机上如何表示浮点数，采用的是 Intel 的标准。它定义了浮点数的格式（包括正负0）与反常值(denormal number)，特殊值(无穷 (Inf) 与非数值(NaN))，以及这些数值的“浮点运算符”；也指明了四种数值舍入规则和五种例外情况

FP32 格式的浮点数(single precision) 在 大部分 C/C++ 系统里就是 float，无论是 x86 CPU 还是 NV 的 GPU 都是支持的。而 FP16则不在 C/C++ 标准里，需要依赖特殊库，而 PyTorch 或 CUDA 里支持：torch.float16 or torch.half

bfloat16 是 Google 在自家 NPU 上支持的，后来 NV 在 A100 上也支持了。是 s7e8，

fp16 可以简记为 s10e5，即尾数精度为10位，指数为5位。实际存储时是 sign, e,s。

Range: ~5.96e−8 (6.10e−5) … 65504

## BFloat 16 
fp16 提出时主要是解决图片处理的问题，并不是专门针对机器学习的，所以它的动态范围很窄，而且要借助 loss scale 技术。 BFloat 16 是想解决这个问题。提供跟 float32 一样的动态范围，属于 FP32 的截断。
![](./imgs/bfloat16.png)

从上图可以清晰看出 从fp32可以快速转为 bfloat16，指数部分不变，只需要把 fraction 部分截断。


## 问题
1. fp16的 range 没太看明白。5 bit不是可以表示 32 么

## 参考资料
1. [fp16-fp32-bfloat166-tf32](https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407)
2. [half precision floating point visualized](https://observablehq.com/@rreusser/half-precision-floating-point-visualized)
