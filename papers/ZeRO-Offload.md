## 总结
1. 在解决的是什么问题？如何训练大模型，解决内存不够的问题
2. 为何成功，标志/准是什么？ 通过Offload 内存和算力到 CPU 上，能支持更大的模型，效果很好，比如1k个节点上支持 1T 的参数训练
3. 在前人基础上的关键创新是什么？最优化的 offload 策略：算力最少，效率最高，通信量最低：CPU 上的高效优化器实现，6倍的 SOTA；One-step delayed parameter update: 这样 CPU 计算和 GPU计算可以重叠
4. 关键结果有哪些？ 能训练更大的模型
5. 有哪些局限性？如何优化？是针对 NLP 里 模型状态和优化器占用内存最大+Adam 优化器设计的
6. 这个工作可能有什么深远的影响？

## 疑问：
1. 论文里提到的：first principle analysis 是啥？
2. 怎么证明的就是最优的？
3. 在 CPU 和 GPU 之间，如何做到让 CPU 和 GPU 之间通信量最小的？
4. 把梯度，优化器状态和优化器计算放到 CPU上，而其他 backward 和 forward 在 GPU上，为何就最终增加了10倍模型大小，而非5倍？
5. 为何在 CPU 上，执行的优化器计算后，就只是 O(M) 的计算量，而非 GPU 上的 O(MB). M: model size, B: batch size.
6. 试试这个在 POD 上的效果？
