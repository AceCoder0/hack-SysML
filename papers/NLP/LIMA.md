1. 在解决的是什么问题？
2. 为何成功，标志/准是什么？
3. 在前人基础上的关键创新是什么？
4. 关键结果有哪些？
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？


## 1 介绍

我们假设 alignment 的过程只是简单地让模型学习和用户交互的风格或者说格式，把 pretrain 阶段学到的知识和能力充分暴露出去。为了验证这个假说，我们精心挑选了 1000 个近似的真实用户 prompt 和高质量回复。其中 750 条社区论坛里的高质量问题和回复，比如 stack exchange 和 wikiHow，基于质量和多样性采样。另外还写了 250 个 prompts 和 response 的例子，同时优化任务的多样性，强调作为 AI 助手的统一的回复风格。最终训练出来了 65B 参数的 LLaMa 模型

## 2 对齐时的数据

定义了 表面、肤浅的对齐假说：模型的知识和能力整个都来自 pretraining 阶段，alignment 教会他当跟人类打交道时，使用哪种风格的子分布(所以是已经会的很多风格里的一种）。假设这种说法是对的，对齐大部分是学习风格，那么这个假说的推论之一是可以使用相对较少的训练集就可以高效 SFT。

最终我们收集的 1k 个 p、r 对里，输出是对齐到一种风格，但是输入（prompts）是多样的。

消融实验揭示了扩大数据量而没有对应扩大 prompt **多样性**时，受益会迅速降低，同时**优化数据**质量时受益很大。尽管没有对话的例子，但是 LIMA 能处理多轮对话，这种能力可以通过在训练集里增加仅仅30条手工的对话就可以迅速提高。


数据开源了：https://huggingface.co/datasets/GAIR/lima


## 7 讨论
有几个限制：
1. 构造这些数据的心智代价比较高，很难扩展
2. 在 decode 阶段的 unlucky sample 或者是对抗样本会导致 weak response。
