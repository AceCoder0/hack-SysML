## 摘要
NLP里有大量的任务，虽然有大量的没标注的文本，但标好的很少，所以我们在标好的上面训练任务比较难。作者提出的方法是先在大量**没标好**的文本上训练一个生成式的预训练语言模型，然后在下游标好的数据上训练一个分辨的微调模型(discriminative fine-tuning)。虽然8、9年前 CV 里早就流行的方法，在 NLP 里一直没有流行起来，是因为 NLP 里没有 ImageNet 那样大规模的标注好的数据。所以思路是把 CV 里的方法用到 NLP 里，使用没标好的数据。GPT 也希望不改变模型，只改变输入的情况下就适配下游任务。它之后的 BERT 效果更好。

## 导言

同样思路效果最好的是 word embedding 模型。

困难：
1. 没有哪种优化方法(optimization objectives)(损失函数）是在大多数任务上都有效地
2. 使得好效果能迁移到很多下游任务上面。

提出半监督的方法：
十年前非常火的思路：我有少量标注好的数据，大量没标注好的数据，怎么把这些大量没标注好的数据用好。但后人把这些方法不叫半监督了，叫自监督方法。

作者在选 RNN 还是 transformer 结构时，选了后者：
1. 认为提供了更结构化的 memory 来处理长文本，导致迁移效果更鲁棒(更好的句子和段落层面的语义信息）
2. 在做迁移的时候，用的任务相关的输入表示

## 相关工作

## 3 框架
在没有标好的数据上做预训练

### 3.1 Unsupervised pre-training
目标函数是训练一个模型，让他的输出跟你的输入长得一样（概率最大）。

使用了多层的 transformer decoder：计算注意力的时候，由于掩码的存在，让当前 i 只看前面的，不看后面的。
