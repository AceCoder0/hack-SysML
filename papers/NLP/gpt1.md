1. 在解决的是什么问题？希望在语言里，也能使用 CV 里的预训练 + 下游任务微调的模式
2. 为何成功，标志/准是什么？希望把 NLP 里大量未被标注的数据用起来
3. 在前人基础上的关键创新是什么？使用 transformer 里的 decoder 来处理数据
4. 关键结果有哪些？跑通了 pretrain + finetuning 的路子
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？


## 摘要
NLP里有大量的任务，虽然有大量的没标注的文本，但标好的很少，所以我们在标好的上面训练任务比较难。作者提出的方法是先在大量**没标好**的文本上训练一个生成式的预训练语言模型，然后在下游标好的数据上训练一个分辨的微调模型(discriminative fine-tuning)。虽然8、9年前 CV 里早就流行的方法，在 NLP 里一直没有流行起来，是因为 NLP 里没有 ImageNet 那样大规模的标注好的数据。所以思路是把 CV 里的方法用到 NLP 里，使用没标好的数据。GPT 也希望不改变模型，只改变输入的情况下就适配下游任务。它之后的 BERT 效果更好。

## 导言

同样思路效果最好的是 word embedding 模型。

困难：
1. 没有哪种优化方法(optimization objectives)(损失函数）是在大多数任务上都有效地
2. 使得好效果能迁移到很多下游任务上面。

提出半监督的方法：
十年前非常火的思路：我有少量标注好的数据，大量没标注好的数据，怎么把这些大量没标注好的数据用好。但后人把这些方法不叫半监督了，叫自监督方法。

作者在选 RNN 还是 transformer 结构时，选了后者：
1. 认为提供了更结构化的 memory 来处理长文本，导致迁移效果更鲁棒(更好的句子和段落层面的语义信息）
2. 在做迁移的时候，用的任务相关的输入表示

## 相关工作

## 3 框架
在没有标好的数据上做预训练

### 3.1 Unsupervised pre-training
目标函数是训练一个模型，让他最大概率地输出跟输入长得一样（概率最大)的单词。

transformer 里有两个东西：

1. 编码器：对第i个元素抽取特征时，它可以看到序列里所有的元素
2. 解码器: 有掩码的存在，当对第i个元素抽取特征时，它只能看到当前元素和之前的元素 
使用了多层的 transformer decoder：计算注意力的时候，由于掩码的存在，让当前 i 只看前面的，不看后面的。

Embedding + Positional Embedding（位置信息编码）

问题: P(u) 是一个单词还是一个句子的概率？

### 3.2 微调(supervised fine-tuning)
此时有两个目标函数：除了上述预训练时根据前k个预测下一个单词，还有给你**完整**的序列，让你预测序列的**标号**

论文里介绍了4种下游任务，把所有任务都构造为了输入序列，经过一样的 transformer，然后再经过 Linear + softmax 层

start、delim 等**特殊字符**必须和普通的文本里的词区分开，不然模型就混淆了。

对于多选题，是判断每个选项的执行度。

## 4 实验

后来 BERT 用了更大的模型，更大的结构，发现效果比 GPT1 效果要好。
