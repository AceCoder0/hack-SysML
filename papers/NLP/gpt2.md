1. 在解决的是什么问题？
2. 为何成功，标志/准是什么？
3. 在前人基础上的关键创新是什么？
4. 关键结果有哪些？
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？

现在主流途径是对一个任务收集数据集，在上面做训练然后做预测。原因是模型的泛化性不好。多任务学习(Multitask Learning)的观点：在一开始训练一个模型的时候，同时看多个数据集，通过多一个损失函数来达到一个模型能在多个任务上使用。这个东西在 NLP 里用的不多，NLP 里用多的是 gpt1 和 bert 那样的模型，在一个大的数据集上做好无监督的预训练，然后做有监督的微调。但还是有两个问题：

1. 每个下游任务都需要重新训练一个模型
2. 收集有标签的数据（有成本）

其实本文里的 multitask learners 和 multitask learning 还是不太一样的 

我们还是在做语言模型，但是在下游任务里，是 zero-shot 的设定：在下游任务里不需要任何标注的信息，也不需要训练模型。

在某些任务上表现不错（在某些比如问答领域里远远不如有监督的方法训练出来的模型）

## 2 方法
不能引入gpt1里那样的特殊文本了。而是设定输入

### 2.1 训练数据

和别的做 ZeroShot 的任务相比
