1. 在解决的是什么问题？如何让对齐的 LLM 产出有害的内容，越狱
2. 为何成功，标志/准是什么？
3. 在前人基础上的关键创新是什么？发现了一种后缀，产出过程不是手工，而是自动使用弹性算法来生成。而且在开源模型上生成的后缀，也能用在闭源的模型上，比如 ChatGPT，Bard 和 Claude 里
4. 关键结果有哪些？
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？


[Example of Jailbreaking LLaMA-2](https://github.com/llm-attacks/llm-attacks/blob/main/demo.ipynb)
