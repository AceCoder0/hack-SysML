1. 在解决的是什么问题？
2. 为何成功，标志/准是什么？
3. 在前人基础上的关键创新是什么？把输入的序列使用 Ring 的方式在多个设备上计算，这样计算和通信可以并行
4. 关键结果有哪些？在非常大的输入上下文上训练时 FLOPs 都是很高效的
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？

Ring Attention 与标准 attention 的区别：
1. 显存高效: 通过增量地计算，分布式分配计算到多个设备上来避免超过平方的显存（咋做到的
2. 可扩展: 可以随着设备的数量增加线性扩展（所以几乎可以无限）



## 疑问
0. 实现里：怎么组成 ring？
1. 和 lightseq: sequence level parallelism 之间的异同点是什么？
2. 内存空间占用有什么帮助吗？是平方关系还是接近线性？
3. 处理 100M tokens 需要超过 1000 GB 显存，这个是怎么算的？
4. 每个块内部的计算复杂度是多少？
5. Blockwise transformers 是什么样的？
6. 它和 flash attention 之间的关系是什么？可以看我的那个 ppt，了解切分规则即可。比如 Q、K、V 各自怎么切分的

