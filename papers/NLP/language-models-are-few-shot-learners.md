1. 在解决的是什么问题？
2. 为何成功，标志/准是什么？
3. 在前人基础上的关键创新是什么？
4. 关键结果有哪些？
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？

## 摘要
近些年的工作说明通过在大语料数据集上做 pre-training，然后在特定任务上做 fine-tuning，在很多 NLP 任务和 benchmarks 里面有潜在收益。尽管在结构上是任务无关的，但是这种方法依然需要上万或者亿的 fine-tuning 所需的任务特定的数据集。但对比起来，人类通常根据一些简单的例子或者简单的指令来执行一个新的语言任务---这是当前 NLP 系统依然在挣扎而无法做到的领域。本文展示了扩大语言模型的参数量，可以极大地提高任务无关性，few-shot 下的性能，一些情况下达到 sota 的 fine-tuning 性能。而且，我们训练 GPT-3，一个自回归语言模型（175B）参数，比之前非稀疏语言模型要大10倍，测试了它在 few-shot setting下的性能。所有任务里，GPT-3 没有使用任何的梯度更新或者 fine-tuning，而是让任务和few-shot 例子单纯是通过文本交互来完成。GPT-3 在很多 NLP 数据集上达到了很强的性能，包括翻译，问答和填空任务，还有其他需要实时推理或者 domain adaption，例如整理文字，句子里使用新词，或者执行3位数算术。同时，我们可以也发现在一些数据集上，GPT-3 依然表现不好的，也有数据集上面临方法问题（在大的web预料上训练）。最终，我们发现GPT-3可以生成一些新文章的样例，让人类评估员都无法分辨出是否是人写的。我们会更通用地讨论此发现的广泛的社会影响。

## 介绍
近些年出现了趋势是在NLP里，朝着 pre-trained 语言表征去发展，越来越多地以任务无关地方式应用。首先是单个layer的表征，通过 word vectors来学习，然后喂给任务有关的结构里，之后是有着多层表征的 RNNs 和 文本状态倍用来组成更强的表征（尽管应用在了任务特定的结构上），最近的 pre-trained rNN 或者 transformer 语言结构被直接 fine-tuned，整个移除了任务特定的结构。

最后的这种方法导致在很多本来很有挑战的 NLP 任务比如阅读理解，问答，文本等其他领域里有很大的进步，而且能通过基于新的架构和算法来持续进步。但是很大的限制是监管结构与任务无关，但是依然需要任务特定的数据集和任务特定的fine-tuning：为了达到在目标任务上高性能，通常需要在上万以上的数据集上做fine-tuning。从以下几个原因来看，移除这个限制比较好。

1. 从实用角度来看，每个新任务需要标注一个大数据集限制了语言模型的应用。从在大量的有用的语言任务，包含修改语法，生成抽象概念的例子，生成小的故事等。这些任务里很难收集大量监督的训练数据集，尤其是当这个过程在每个新任务上都需要进行

2. 潜在的利用训练数据里伪相关性的潜能会随着模型表达能力和训练分布的变窄而增加。这会导致 pre-training 加 fine-tuning 的范式下，会有问题，模型被设计足够大来吸收 pre-training 过程中的信息，但是在非常狭窄的任务分布上做 fine-tuning。例如这个工作发现更大的模型并不是总能在分布之外泛化的更好。这也是证据表明在这种范式下取得的泛化可能很槽糕，原因是模型在训练分布上太特定了，无法在它之外泛化的很好。而且，特定benchmark上的 fine-tuned 模型的性能，即使有人类级别，也可能夸大了在任务上的实际性能。

3. 人类不需要大量监督的预料来学习大部分语言模型--很简单的自然语言（告诉我下面这个句子描述的是开心还是伤心）或者至多就是少量例子（比如以下是两个很勇敢的例子；请给我第三个例子）就足够人类去执行新的任务，而且有一定程度的完成度。除了现存 NLP 技术里的概念限制，这种能力有很实用的长处：容许人类无缝把多个任务和技能混合在一起或者切换，比如在一个对话里面执行加法。为了更有用，我们希望某一天 NLP 系统也能有这种普遍性。

一个潜在的克服此问题的方法是 meta-learning -- 在语言模型的上下文里，代表模型在训练时，发展了很广泛的技能和模式识别能力，因此在推理时快速使用或者识别出期望的任务(图1.1）

![](imgs/language-models-meta-learning.png)

图1.1：语言模型的 meta-learning。我们使用”in-context learning“ 来描述这个过程的内部loop，在每个序列里 fwd 过程中发生。上图里的序列并不能代表模型在 pre-training 过程中能看到的，但是是特意用来展示在单个序列里，嵌入的重复的子任务。

最近的一些工作尝试通过”in-context learning“来做，对 pre-trained 语言模型，使用文本输入来作为任务规格的形式：模型是在自然语言指令为条件，需要少量这个任务的例子，之后就可以完成其他此任务的例子，用简单地预测后续序列来完成此任务。

尽管展示了一些潜在的可能，但是此方法依然不如 fine-tuning好。

在语言模型的上下文里，这种 in-context learning 有时也叫做”zer-shot 迁移“，，但是这个词有歧义：”zero-shot“指的是不需要执行梯度更新，但是需要在推理时提供一些例子给模型，所以并不是从零样本学习。为了避免歧义，叫做”meta-learning“来捕获 inner-loop、outer-loop 结构，”in context-learning" 代表 meta-learning 里的内部 loop。

图1.2：大语言模型高效地使用 in-context 信息。展示了移除句子里某个随机符号的模型，使用或者不使用 NLP 任务描述。更陡的”in-context learning 曲线“代表大模型的根据上下文信息学习任务的能力提高了。我们在很多任务上都能看到类似性质的行为。

![](imgs/larger-models-make-increasingly-efficient-useofin-context-information.png)

另外一个 LM 里的趋势是提供了一种可能。近些年，transformer 模型的容量增大了，从100 m 到 300m，再到1.5b，8b，11b，17b。每一次增加都会在下游任务上涨点，这证明 log loss，把下游任务关联了起来，随着scale增大而带来收益。因此很可能随着 scale 增大，in-context learning的能力也会随之增加。

本文里，我们测试了 175b的自回归lm，叫做 GPT-3
## 
