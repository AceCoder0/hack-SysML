

1. 在解决的是什么问题？文档识别中的子类：手写体字母识别
2. 为何成功，标志/准是什么？ 不需要像之前的方法一样，需要很多人工介入/设计/处理。对字母的大小，变体等有鲁棒性。
3. 在前人基础上的关键创新是什么？ 更多依赖于从数据里自动学习，而非手工针对任务做的启发式设计。这样一个统一结构的模型可以代替过去好几个独自设计的模块
4. 关键结果有哪些？
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？

## 为什么会出现这个技术？
1. 算力：更便宜、算力更强的机器出现，让“暴力破解”式的数值计算方法成为可能
2. 数据集：解决这类问题有越来越大的市场，而且数据集越来越大。这样可以依靠数据来做
3. 算法：更强的机器学习算法，可以处理高纬度的数据

## Gradient-Based Learning
在CS 中，给由多个变量的函数解出最优解是很多问题的根源。 GRadient-Based 学习是基于相比离散函数，连续函数上解出最优解更容易。

## Gradient Back-Propagation
基于梯度的方法早在1950年就提出了，但是只在线性系统里使用。直到三个事件发生后，才被广泛应用到复杂的机器学习里：

1. 尽管损失函数中的局部最优值存在，但不会在实践中成为大问题。
2. back-propagation 算法在非线性系统里使用起来
3. 在 多层 NN 中，基于 back-propagation 和 sigmoid 解决了一些复杂的学习任务

这些思路最早是在60年代的控制论里提出的。

图片有很强的2D局部性：在空间或临时出现在附近的像素/变量都是高度相关的。 CNN 就是发挥局部性

CNN 对于输入的移位和变形有较强的鲁棒性.当特征被检测出后（权重稳定），它的具体位置就不那么重要了。只有相对于其他feature的相对位置是相关的。
## 吃惊
居然有119个参考文献。2/3的内容是介绍这个方法与其他众多方法的效果对比
