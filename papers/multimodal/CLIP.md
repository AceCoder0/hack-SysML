1. 在解决的是什么问题？
2. 为何成功，标志/准是什么？
3. 在前人基础上的关键创新是什么？
4. 关键结果有哪些？
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？


没看懂的是为什么 clip github 首页上的例子里，两个矩阵相乘之后，就可以算相似度呢？而且输出的就是单词了，而不是字母或者没有含义的

```

similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
values, indices = similarity[0].topk(5)
```

## 摘要
我们证明简单的预训练：预测图片上标题就可以是高效并且可扩展的方式来学会 SOTA 的 image 表征，在 4亿图文对上就可以做。预训练之后，就可以使用自研语言来指代学到的视觉概念，产生的 zero-shot 的迁移到下游任务上。们在30个不同的已有的 cv 数据集，比如 OCR，视频里的动作识别，地理位置，其他各种细粒度的物体分类。这个模型很容易迁移到更多类任务上
