## 3 训练基础设施
使用 TPUv5e 和 TPUv4 来训练。

TPUv4 加速器部署在多个有 4096 个芯片的 “SuperPods” 上，每个连接在特定的光学交换机上，可以在10s左右自动重新配置 4x4x4 的芯片到任意的 3D 拓扑结构。对于 Gemini Ultra，我们决定保留少量的正方形来作为热备份和滚动维护。

TPU 通常主要是通过告诉的芯片间互联设备来通信的，但是 Gemini Ulta 的规模下，我们把多个数据中心里的 SuperPods 通过 Google 的集群内和集群间网络互联。网络延迟和带宽足够支撑常用的同步训练范式，在 superpod 内部进行(model parallelism,可见 TP 可以最大到 4096?)，而 superpods 之间进行数据并行

有 GSPMD 切分器，MegaScal XLA 编译器。

在如此大的规模上维护高的 goodput 几乎不可能用传统的间断性地 checkpoint 权重到可持久化的集群存储上去。我们使用了模型状态的内存冗余拷贝，遇到任何故障，就快速地从一个完整的模型副本里直接恢复。相比于 PaLM-2，这种方法的恢复时间更短（尽管使用了更大量的训练资源）。

我们遇到解决的问题之一是“Silent Data Corruption"。监管很少遇到，但是在 Gemini 的规模下，我们每一到两周就会遇到。我们完全确定性的基础设施，让我们可以快速定位到根源（包括硬件故障）。
