1. 在解决的是什么问题？
2. 为何成功，标志/准是什么？
3. 在前人基础上的关键创新是什么？第一次把 instruction tuning 思路引入到 LMM 里，使得效果更好
4. 关键结果有哪些？把 GPT-4产出的视觉指令tuning data、模型和代码都开源了
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？

## 摘要
使用机器生成的追随指令数据，能用来提高 LLM 在 zero-shoft 场景下新任务上的能力，但是这个思路很少在多模态领域里使用。在本文里，我们会第一次使用 GPT-4 纯语言模型来产生多模态的语言-图片指令追随数据。通过在这种数据上进行指令调优(instruction tuning)，我们做出了 LLaVA：Large Language and Vision Assistant，一个端到端训练过的大多模态模型，可以把视觉encoder和LLM结合起来做通用的视觉和语言理解。 Science QA 上 fine-tuned 之后，效果更佳。

## 1 介绍
AI 领域的核心抱负之一是开发一个通用的助手，可以高效跟随多模态的视觉-语言的指令，把人类的意图和众多实际任务对齐。

社区里出现了一批开发语言模型修正过的基础视觉模型，有在开放视觉理解上的很强能力，比如分类，检测，分割和标题提取，也有视觉生成和编辑。Computer Vision in the Wild 指开发可迁移的基座模型、系统来轻松适配到真实世界里的视觉任务上。这些工作的特点是使用一个大的视觉模型来独立解决每个任务，而任务的指令是隐式地被考虑在了模型设计里面。语言知识用来描述图片内容。这虽然让语言可以扮演一个更重要的把**视觉图像**映射到**语言语义**里面去
