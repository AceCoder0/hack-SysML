1. 在解决的是什么问题？ 内部协变量偏移
2. 为何成功，标志/准是什么？利用 BN 后，训练速度变快，还有正则化效果
3. 在前人基础上的关键创新是什么？
4. 关键结果有哪些？
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？

训练过程中每一层对上一层输入的数据分布敏感，因此学习率比较低，而且对参数初始化很敏感。这种现象叫内部协变量偏移（Internal Covariate Shift)

之前研究结果表明，如果网络的输入经过 



## 问题
1. 如何保证数据经过白化之后，其表达能力不受限制？引入了 alpha 和 beta，这样可以还原回原始的数值。这俩参数当然也是单个维度粒度的


数据白化(whitened/whiteining)：原始图像相邻元素像素值具有高度相关性，所以图像数据信息冗余。白化的作用：

1. 减少特征之间的相关性
2. 让特征具有相同的方差

如何白化？想办法让输入的均值为0，方差是1，这样输入变量之间去除了相关性。所以感觉 BN 就是神经网络里的数据白化方法？

