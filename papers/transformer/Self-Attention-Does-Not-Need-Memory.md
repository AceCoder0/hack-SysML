1. 在解决的是什么问题？ 让 O(n^2) 显存降下来
2. 为何成功，标志/准是什么？这种 attention 计算下，显存跟序列长度是 O(1)，即常数的关系，而 self-attention 需要 O(logn) 显存。复杂度没变，依然 O(n^2)
3. 在前人基础上的关键创新是什么？
4. 关键结果有哪些？
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？
