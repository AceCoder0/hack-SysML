1. 在解决的是什么问题？从预训练好的 DNN 里迁移知识到新的 DNN 里，定义了一种新的蒸馏方法
2. 为何成功，标志/准是什么？学生学的快，比老师表现好，学生的能力可迁移到别的任务
3. 在前人基础上的关键创新是什么？
4. 关键结果有哪些？它提出的把两个层之间 feature 之间的 inner product 当作知识的方法，在三方面有效果：1. 更快的优化效果(fast optimization) 2.小网络上效果好，收敛快 3.迁移学习，预训练模型可以在深的网络，在大的数据集上训练，而之后的小模型在不同任务，在小的数据集上可以表现很好
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？


## 1. Introduction

## 2. 相关工作 
Fast optimization: 一种技术，让研究院基于初始化权重，能更快找到局部或者全局最优解。


有不少初始化方法：Xavier initialization. 但这种简单初始化方法在非常深的网络里效果很差，有了好的初始化，训练就能从合适的点出发，更快达到全局最优。SGD 无法从马鞍(Saddle)型的点逃脱出来，所以有了其他方法。

Knowledge Transfer:
Hinton 首创的

Transfer Learning:
修改已经训练好的网络的参数，来适配到新的任务。比如把这个训练好的网络当作 feature extraction，可以把它freeze 或者 finetune，然后增加一个末端的分类器来做新任务。这个classifier 可以随机初始化，然后用一个小的学习率来学习。Fine-tuning 通常比从零开始训练要好，因为已经有大量的知识了。

## 3. 方法
主要两部分：

1. 知识的定义
2. 如何迁移

## 启发
1. 在 deep 和 thin 的网络上，蒸馏需要让每一层都参与，而非只考虑最后一层

## 问题
