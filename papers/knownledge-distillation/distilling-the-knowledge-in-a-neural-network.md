1. 在解决的是什么问题？如何把几个模型一起服务(ensemble of models)的方式，通过蒸馏技术搞到一个小模型里，这样方便部署，而且非常经济
2. 为何成功，标志/准是什么？
3. 在前人基础上的关键创新是什么？在 Caruana 提出的 Model compression 这种把多个模型的能力搞成一个小模型的基础上，使用了不同的模型压缩技术：蒸馏。
4. 关键结果有哪些？可以通过蒸馏让小模型能力提升很高（是语音场景）。提出了一种**新的 ensemble 方法**，由一个 full model 和多个专家组成。专家解决full model 搞不定的一些样本。这种方法比 MOE 要训练友好：能并行训练，速度比较快
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？

## 介绍
很多昆虫有一种幼虫(larval)形态：为了从环境中吸取能量和营养而做了优化，也有一种完全不同的成年体形态，给旅行和繁衍做了优化。类比到机器学习领域，训练和部署的需求不太一样：

训练：需要从大量冗余的数据集里学习抽取结构（比如语音和物体识别），但不需要实时处理，会消耗很多算力

部署：需要部署到很多地方，有很多延迟和计算开销方面的限制

跟昆虫类似，我们能否训练一个笨重的模型，而使用“蒸馏”方法来从它里面把知识迁移到小模型里来更适合部署？之前 Rich Caruana 的 Model Compression 就是做这个方向的。已经证明了一个大的聚合的多模型可以把学到的知识迁移到一个小模型里

## 具体方法

一种方法是把笨模型的输出类别的概率，当作训练小模型的“软目标”，当软目标提供高墒，这样每个训练样本提供了比 hard target 更多的信息，而且训练样本间的偏差会更小，所以小模型通常可以用更少的数据集，更大的学习率来学习。

之前的 Model Compression(Caruana) 是把最后 softmax 的输入的 logits 作为小模型的训练目标，让小模型的 logits 和 大模型的 logits 差异尽量小。这个 logits 里包含了多个类别的信息，而非 softmax 之后加强了差异的信息

## 启发
如果能找到一些大模型到小模型的路子，比如对应关系，或者范式，能让大模型产生出小模型就好了

## 问题
1. 预训练和蒸馏的关系？
2. 这种方法能扩展到其他任务吗？是否对任务类型和数据敏感？后来人的论文里提到，本论文的方法虽然提高了精度，但是有几个弊端: a. 优化非常深的网络就有问题了，后来人发现可以考虑更多层，而非 Hinto 它们提到的最后一层. b.所谓知识是什么？蒸馏的效果对知识的定义非常敏感。本论文认为实际老师教授的解决问题的方法，是DNN 里两层之间的feature 关系
3. 为啥现在 Transformer 上好像还是无法从大模型搞出来小模型
