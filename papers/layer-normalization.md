1. 在解决的是什么问题？ 把 BN 的加速效果引入到 RNN 里
2. 为何成功，标志/准是什么？
3. 在前人基础上的关键创新是什么？计算更简单，并不是每个维度上的，而是某一层共享 alpha 和 beta
4. 关键结果有哪些？
5. 有哪些局限性？如何优化？
6. 这个工作可能有什么深远的影响？

layernorm 的输入假设是 [b,s,h]，那么他中间产出的是 mean([b, s])，variance([b, s])，即是对 hidden 这个纬度做平均。而输出大小不变，依然是 [b, s, h]，但是 h 这个纬度上的值的范围会被归一化

